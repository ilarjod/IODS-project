# Clustering and classification

```{r warning=FALSE, message=FALSE}
library("tidyverse")
library("ggplot2")
library("dplyr")
library("MASS")
library("corrplot")
data("Boston")
Palette <- c("#555555", "#009E73", "#56B4E9", "#E69F00", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

```{r warning=FALSE, message=FALSE}
str(Boston)
pairs(Boston)

cor_matrix<-cor(Boston) %>% round(digits = 2)
cor_matrix

corrplot(cor_matrix, method="circle", type="lower", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```


```{r warning=FALSE, message=FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)
```
Scaling means  subtracting the column means from the corresponding columns and dividing the difference with standard deviation:

$$
scaled(x) = \frac{x-means(x)}{sd(x)}
$$

```{r warning=FALSE, message=FALSE}
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)

# save the scaled crim as scaled_crim
scaled_crim <- boston_scaled$crim

# summary of the scaled_crim
summary(scaled_crim)

# create a quantile vector of crim and print it
bins <- quantile(scaled_crim)
bins

crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Training & testing:
```{r warning=FALSE, message=FALSE}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
n
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
ind
# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]
test

# save the correct classes from test data
correct_classes <- test$crime

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2.5)

lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
correct_classes
```
```{r warning=FALSE, message=FALSE}
data(Boston)
Bostonscaled <- scale(Boston)
# euclidean distance matrix
dist_eu <- dist(Bostonscaled)

# look at the summary of the distances
summary(dist_eu)
# manhattan distance matrix
dist_man <- dist(Bostonscaled, method = "manhattan")

# look at the summary of the distances
summary(dist_man)

# k-means clustering
km <-kmeans(dist_eu, centers = 15 )

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

```

```{r warning=FALSE, message=FALSE}
# MASS, ggplot2 and Boston dataset are available
set.seed(123)

# euclidean distance matrix
dist_eu <- dist(Boston)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})

# visualize the results
plot(1:k_max, twcss, type='b')

# k-means clustering
km <-kmeans(dist_eu, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```
