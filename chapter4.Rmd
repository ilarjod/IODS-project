# Clustering and classification

```{r warning=FALSE, message=FALSE}
library("tidyverse")
library("ggplot2")
library("dplyr")
library("MASS")
library("corrplot")

data("Boston")
Palette <- c("#555555", "#009E73", "#56B4E9", "#E69F00", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

This week's data is about housing values in [Boston](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).


```{r warning=FALSE, message=FALSE}
str(Boston)
pairs(Boston)
```

So there are 506 observations and 14 variables. The plot is not very informative. It's easier to spot high correlations with a correlogram, where a bigger and darker ball indicates higher correlation.

```{r warning=FALSE, message=FALSE}
cor_matrix<-cor(Boston) %>% round(digits = 2)
corrplot(cor_matrix, method="circle", type="lower", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
```

The variables are all continuous so the data can be scaled with one command for the further analysis. 

Scaling here means subtracting the column means from the corresponding columns and dividing the difference with standard deviation:

$$
scaled(x) = \frac{x-means(x)}{sd(x)}
$$

```{r warning=FALSE, message=FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)
```

```{r warning=FALSE, message=FALSE}
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# save the scaled crim as scaled_crim
scaled_crim <- boston_scaled$crim

# summary of the scaled_crim
summary(scaled_crim)

# create a quantile vector of crim and print it
bins <- quantile(scaled_crim)


crime <- cut(scaled_crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

I broke the crime variable to 4 classes: low, med_low, med_high and high, each representing one quintile of the values. This is done because I can analyse the clustering only when the target variable is categorical.


Next I sample 80% of the observations for the train set and leave the remaining 20% for the test set.

```{r warning=FALSE, message=FALSE}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)


```

In my LDA (linear disrciminant analysis) the categorical crime variable is the target and all the other variables are predictor variables. I fit the lda model to the train set and try to predict the classes in the test set. 

```{r warning=FALSE, message=FALSE}
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

We can see that variable rad (index of accessibility to radial highways) clearly has something to do with the high crime cluster.

```{r warning=FALSE, message=FALSE}

lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

The model works OK, even though some classes are predicted wrong but that is to be expected.


Moving on to k-means clustering. I recall the data and scale it to examine distance properties of the data. I check out the euclidean (normal way of understanding distances) and manhattan (as if you lived in manhattan and had to move via the grid of streets) distances.

```{r warning=FALSE, message=FALSE}
data("Boston")
Bostonscaled <- scale(Boston)
# euclidean distance matrix
dist_eu <- dist(Bostonscaled)
# look at the summary of the distances
summary(dist_eu)
# manhattan distance matrix
dist_man <- dist(Bostonscaled, method = "manhattan")

# look at the summary of the distances
summary(dist_man)

# k-means clustering
km <-kmeans(dist_eu, centers = 10 )

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```

Boston data with clusters. Then I think research what is an optimal amount of clusters for the data.

```{r warning=FALSE, message=FALSE}
# MASS, ggplot2 and Boston dataset are available
set.seed(27)

# euclidean distance matrix
dist_eu <- dist(Boston)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
```

 WCSS (within cluster sum of squares) drops the most at 2 centers.

```{r warning=FALSE, message=FALSE}
# visualize the results
plot(1:k_max, twcss, type='b')

# k-means clustering
km <-kmeans(dist_eu, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```
(Again very poor plot but i cannot do better.)